{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Mesh Master Bot v2.0 - Training Notebook\n",
        "\n",
        "**Optimized fine-tuning of Llama-3.2-1B-Instruct for Meshtastic mesh networks**\n",
        "\n",
        "## Key Improvements from v1.0:\n",
        "- ‚úÖ Using **Llama-3.2-1B** (not gemma3:270m!)\n",
        "- ‚úÖ 10x **lower learning rate** (0.00002 vs 0.0002)\n",
        "- ‚úÖ **Only 1 epoch** to prevent overfitting\n",
        "- ‚úÖ Added **regularization** (dropout, weight decay)\n",
        "- ‚úÖ **Early stopping** to prevent memorization\n",
        "- ‚úÖ **Repeat penalty** in final model\n",
        "\n",
        "## Requirements:\n",
        "- Colab GPU (T4, A100, or V100)\n",
        "- HuggingFace account with Llama access\n",
        "- Training data in ShareGPT format\n",
        "\n",
        "**Estimated time:** 2-3 hours on T4, 45-60min on A100"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã Step 1: Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (takes ~5 minutes)\n",
        "!pip install -q -U \\\n",
        "  git+https://github.com/axolotl-ai-cloud/axolotl.git@main \\\n",
        "  transformers==4.46.2 \\\n",
        "  datasets==3.0.2 \\\n",
        "  accelerate==1.1.1 \\\n",
        "  peft==0.13.2 \\\n",
        "  bitsandbytes==0.44.1 \\\n",
        "  flash-attn==2.7.0.post2 \\\n",
        "  wandb \\\n",
        "  huggingface_hub"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace (need access to Llama-3.2-1B-Instruct)\n",
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "hf_token = getpass.getpass(\"Enter your HuggingFace token: \")\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"‚úÖ Logged in to HuggingFace!\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "print(\"‚úÖ Logged in to W&B!\")"
      ],
      "metadata": {
        "id": "wandb_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 2: Upload Training Data\n",
        "\n",
        "Upload your `mesh_conversations.jsonl` file using the file browser on the left.\n",
        "\n",
        "**Format:** ShareGPT style with `conversations` key:\n",
        "```json\n",
        "{\"conversations\": [{\"from\": \"human\", \"value\": \"How do I relay to alice?\"}, {\"from\": \"gpt\", \"value\": \"alice <message>\"}]}\n",
        "```"
      ],
      "metadata": {
        "id": "upload_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify training data\n",
        "import json\n",
        "\n",
        "data_path = \"/content/mesh_conversations.jsonl\"\n",
        "\n",
        "# Count examples\n",
        "with open(data_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print(f\"üìä Found {len(lines)} training examples\")\n",
        "\n",
        "# Show first example\n",
        "example = json.loads(lines[0])\n",
        "print(\"\\nüìù Sample conversation:\")\n",
        "for turn in example['conversations']:\n",
        "    role = \"User\" if turn['from'] == 'human' else \"Assistant\"\n",
        "    print(f\"{role}: {turn['value']}\")"
      ],
      "metadata": {
        "id": "verify_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Step 3: Create Training Config"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/mesh-master-1b-v2.yaml\n",
        "# Mesh Master Bot v2.0 - Optimized for Anti-Overfitting\n",
        "base_model: meta-llama/Llama-3.2-1B-Instruct\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "trust_remote_code: true\n",
        "\n",
        "# QLoRA Settings\n",
        "load_in_4bit: true\n",
        "adapter: qlora\n",
        "lora_r: 8\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.15\n",
        "lora_target_linear: true\n",
        "lora_target_modules:\n",
        "  - q_proj\n",
        "  - k_proj\n",
        "  - v_proj\n",
        "  - o_proj\n",
        "  - gate_proj\n",
        "  - up_proj\n",
        "  - down_proj\n",
        "\n",
        "# Sequence\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "# Batch Size\n",
        "micro_batch_size: 2\n",
        "gradient_accumulation_steps: 16\n",
        "eval_batch_size: 2\n",
        "num_epochs: 1\n",
        "\n",
        "# Learning Rate - CRITICAL CHANGES!\n",
        "optimizer: adamw_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.00002      # 10x lower!\n",
        "warmup_steps: 50\n",
        "warmup_ratio: 0.05\n",
        "weight_decay: 0.01          # Added regularization\n",
        "\n",
        "# Precision\n",
        "bf16: auto\n",
        "fp16: false\n",
        "tf32: true\n",
        "flash_attention: true\n",
        "gradient_checkpointing: true\n",
        "\n",
        "# Dataset\n",
        "datasets:\n",
        "  - path: /content/mesh_conversations.jsonl\n",
        "    type: sharegpt\n",
        "    conversation: conversations\n",
        "\n",
        "val_set_size: 0.15\n",
        "\n",
        "# Evaluation & Checkpointing\n",
        "evaluation_strategy: steps\n",
        "eval_steps: 50\n",
        "save_steps: 100\n",
        "logging_steps: 5\n",
        "output_dir: /content/outputs\n",
        "save_strategy: steps\n",
        "save_total_limit: 3\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping_patience: 3\n",
        "load_best_model_at_end: true\n",
        "metric_for_best_model: eval_loss\n",
        "greater_is_better: false\n",
        "\n",
        "# Tokens\n",
        "special_tokens:\n",
        "  bos_token: \"<|begin_of_text|>\"\n",
        "  eos_token: \"<|end_of_text|>\"\n",
        "  unk_token: \"<|unk|>\"\n",
        "\n",
        "chat_template: llama3\n",
        "seed: 42\n",
        "\n",
        "# Logging\n",
        "wandb_project: mesh-master-bot-v2\n",
        "wandb_name: mesh-1b-colab"
      ],
      "metadata": {
        "id": "write_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 4: Start Training\n",
        "\n",
        "**This will take 2-3 hours on T4, 45-60min on A100**"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "!accelerate launch -m axolotl.cli.train /content/mesh-master-1b-v2.yaml"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 5: Export Model for Ollama"
      ],
      "metadata": {
        "id": "export"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge LoRA adapters into base model\n",
        "!python -m axolotl.cli.merge_lora \\\n",
        "  /content/mesh-master-1b-v2.yaml \\\n",
        "  --lora_model_dir=/content/outputs \\\n",
        "  --load_in_4bit=False \\\n",
        "  --load_in_8bit=False"
      ],
      "metadata": {
        "id": "merge_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to GGUF for Ollama\n",
        "!pip install -q llama-cpp-python gguf\n",
        "\n",
        "!python /usr/local/lib/python3.10/dist-packages/llama_cpp/llama_cpp/convert-hf-to-gguf.py \\\n",
        "  /content/outputs/merged \\\n",
        "  --outfile /content/mesh-master-bot-v2.gguf \\\n",
        "  --outtype q8_0"
      ],
      "metadata": {
        "id": "convert_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Modelfile\n",
        "%%writefile /content/Modelfile\n",
        "FROM /content/mesh-master-bot-v2.gguf\n",
        "\n",
        "# Anti-repetition and length controls\n",
        "PARAMETER temperature 0.5\n",
        "PARAMETER repeat_penalty 1.15\n",
        "PARAMETER num_predict 200\n",
        "PARAMETER top_p 0.85\n",
        "PARAMETER top_k 30\n",
        "\n",
        "# System prompt optimized for mesh networks\n",
        "SYSTEM You are Mesh Master, an AI assistant for Meshtastic mesh networks. Keep responses concise (ideally under 160 characters) for LoRa bandwidth efficiency. Be accurate, helpful, and direct. Never repeat yourself."
      ],
      "metadata": {
        "id": "create_modelfile"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files to your computer\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading model files...\")\n",
        "files.download('/content/mesh-master-bot-v2.gguf')\n",
        "files.download('/content/Modelfile')\n",
        "print(\"‚úÖ Done! Import to Ollama with: ollama create mesh-master-bot-v2 -f Modelfile\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: Test the Model (Optional)"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick inference test\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/outputs/merged\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def test_prompt(question):\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.5,\n",
        "        repetition_penalty=1.15,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(f\"Length: {len(response)} chars\\n\")\n",
        "\n",
        "# Test questions\n",
        "test_prompt(\"How do I relay to alice?\")\n",
        "test_prompt(\"What's a meshtastic router?\")\n",
        "test_prompt(\"Hey are you there?\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

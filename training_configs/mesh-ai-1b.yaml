# Mesh-AI 1B Model Fine-Tuning Configuration
#
# Fine-tunes Llama-3.2-1B-Instruct for Meshtastic mesh networking tasks
# using QLoRA (4-bit quantization + LoRA adapters)
#
# Hardware Requirements:
#   - GPU: 24GB VRAM (RTX 3090/4090, A5000, or better)
#   - RAM: 32GB+ recommended
#   - Disk: 10GB for model + checkpoints
#
# Training Time: ~10-12 hours on RTX 3090
#
# Usage:
#   accelerate launch -m axolotl.cli.train mesh-ai-1b.yaml

# Base Model
base_model: meta-llama/Llama-3.2-1B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# Trust remote code (required for some models)
trust_remote_code: true

# QLoRA Configuration
load_in_8bit: false
load_in_4bit: true
strict: false

# LoRA Adapter Settings
adapter: qlora
lora_model_dir:
lora_r: 16                    # LoRA rank (16-32 typical for 1B models)
lora_alpha: 32                # LoRA alpha (2x rank is common)
lora_dropout: 0.05
lora_target_linear: true      # Target all linear layers
lora_fan_in_fan_out:
lora_target_modules:          # Specific attention modules
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Sequence Length
sequence_len: 4096            # Max context for mesh commands + responses
sample_packing: true          # Pack multiple samples per sequence
pad_to_sequence_len: true

# Batch Size & Gradient Accumulation
# Effective batch size = micro_batch_size * gradient_accumulation_steps
micro_batch_size: 4
gradient_accumulation_steps: 8
eval_batch_size: 4
num_epochs: 3

# Learning Rate & Scheduler
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 0.0002
warmup_steps: 100
warmup_ratio: 0.03
weight_decay: 0.0

# Training Precision
bf16: auto                    # Use BF16 if available
fp16: false
tf32: true                    # Use TensorFloat-32 on Ampere GPUs

# Flash Attention (highly recommended for speed)
flash_attention: true
flash_attn_cross_entropy: true
flash_attn_rms_norm: true

# Gradient Checkpointing (saves memory)
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true

# Dataset Configuration
datasets:
  - path: data/training/train.jsonl
    type: sharegpt
    conversation: conversations

# Validation Dataset
val_set_size: 0.1
dataset_prepared_path: data/training/prepared

# Evaluation Strategy
evaluation_strategy: steps
eval_steps: 100
save_steps: 200
logging_steps: 10

# Save Strategy
output_dir: ./outputs/mesh-ai-1b-qlora
save_strategy: steps
saves_per_epoch: 3
save_total_limit: 4           # Keep only 4 latest checkpoints

# Hub Settings (optional - for uploading to HuggingFace)
hub_model_id: mesh-ai-1b
hub_strategy: checkpoint
push_dataset_to_hub: false

# Special Tokens (Llama 3 format)
special_tokens:
  bos_token: "<|begin_of_text|>"
  eos_token: "<|end_of_text|>"
  unk_token: "<|unk|>"

# Chat Template (Llama 3.2 Instruct format)
chat_template: llama3

# Advanced Settings
ddp_timeout: 3600
ddp_find_unused_parameters: false
group_by_length: true

# Logging & Monitoring
wandb_project: mesh-ai-1b-training
wandb_entity:
wandb_watch:
wandb_name: mesh-ai-1b-qlora-run1
wandb_log_model: checkpoint

# Debug Settings (disable for production)
debug:
deepspeed:
fsdp:
fsdp_config:

# Early Stopping (optional)
early_stopping_patience: 5

# Memory Optimization
max_memory:
low_cpu_mem_usage: true

# Miscellaneous
seed: 42
max_steps:
evals_per_epoch: 4
batch_size:
